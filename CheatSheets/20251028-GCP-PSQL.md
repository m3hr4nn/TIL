# GCP PostgreSQL Database Interview Cheat Sheet

## Cloud SQL for PostgreSQL

**Cloud SQL**: Fully managed relational database service
**Instance**: PostgreSQL database server
**Database**: Schema within an instance
**High Availability**: Multi-zone with automatic failover
**Read Replicas**: Horizontal scaling for read workloads
**Backups**: Automated and on-demand backups
**Point-in-Time Recovery**: Restore to any point within retention period

## Creating Instances

### gcloud CLI
```bash
# Create PostgreSQL instance
gcloud sql instances create my-postgres \
    --database-version=POSTGRES_15 \
    --tier=db-f1-micro \
    --region=us-central1 \
    --root-password=your-password \
    --storage-size=10GB \
    --storage-type=SSD

# With High Availability
gcloud sql instances create my-postgres-ha \
    --database-version=POSTGRES_15 \
    --tier=db-n1-standard-2 \
    --region=us-central1 \
    --availability-type=REGIONAL \
    --root-password=your-password

# List instances
gcloud sql instances list

# Describe instance
gcloud sql instances describe my-postgres

# Delete instance
gcloud sql instances delete my-postgres
```

### Terraform
```hcl
resource "google_sql_database_instance" "postgres" {
  name             = "my-postgres-instance"
  database_version = "POSTGRES_15"
  region           = "us-central1"

  settings {
    tier = "db-n1-standard-2"
    
    backup_configuration {
      enabled            = true
      start_time         = "03:00"
      point_in_time_recovery_enabled = true
      transaction_log_retention_days = 7
    }
    
    ip_configuration {
      ipv4_enabled    = true
      private_network = google_compute_network.private_network.id
      require_ssl     = true
    }
    
    database_flags {
      name  = "max_connections"
      value = "100"
    }
    
    insights_config {
      query_insights_enabled  = true
      query_string_length     = 1024
      record_application_tags = true
    }
  }
  
  deletion_protection = true
}

resource "google_sql_database" "database" {
  name     = "myapp"
  instance = google_sql_database_instance.postgres.name
}

resource "google_sql_user" "users" {
  name     = "appuser"
  instance = google_sql_database_instance.postgres.name
  password = var.db_password
}
```

## Connecting to Cloud SQL

### Public IP Connection
```bash
# Using Cloud SQL Proxy
cloud_sql_proxy -instances=PROJECT:REGION:INSTANCE=tcp:5432

# Then connect locally
psql "host=127.0.0.1 port=5432 dbname=mydb user=postgres"
```

### Private IP Connection
```bash
# Connect from same VPC
psql "host=PRIVATE_IP dbname=mydb user=postgres"
```

### Connection String
```python
# Python example
import psycopg2

conn = psycopg2.connect(
    host="INSTANCE_IP",
    database="mydb",
    user="postgres",
    password="password",
    port=5432,
    sslmode="require"
)
```

### Using IAM Authentication
```python
import google.auth
from google.auth.transport.requests import Request
from google.oauth2 import service_account
import psycopg2

# Get access token
credentials, project = google.auth.default()
credentials.refresh(Request())

conn = psycopg2.connect(
    host="INSTANCE_IP",
    database="mydb",
    user="user@project.iam",
    password=credentials.token,
    sslmode="require"
)
```

## Database Management

### Create Database
```bash
# Via gcloud
gcloud sql databases create mydb \
    --instance=my-postgres

# Via psql
CREATE DATABASE mydb;
```

### User Management
```bash
# Create user
gcloud sql users create appuser \
    --instance=my-postgres \
    --password=secure-password

# List users
gcloud sql users list --instance=my-postgres

# Change password
gcloud sql users set-password postgres \
    --instance=my-postgres \
    --password=new-password
```

### PostgreSQL Commands
```sql
-- List databases
\l

-- Connect to database
\c mydb

-- List tables
\dt

-- List schemas
\dn

-- Describe table
\d table_name

-- List users/roles
\du

-- Grant privileges
GRANT ALL PRIVILEGES ON DATABASE mydb TO appuser;
GRANT SELECT, INSERT, UPDATE ON TABLE users TO appuser;

-- Create schema
CREATE SCHEMA app_schema;

-- Set search path
SET search_path TO app_schema, public;
```

## Backups and Recovery

### Automated Backups
```bash
# Configure backup window
gcloud sql instances patch my-postgres \
    --backup-start-time=03:00

# Enable point-in-time recovery
gcloud sql instances patch my-postgres \
    --enable-point-in-time-recovery \
    --retained-transaction-log-days=7
```

### Manual Backups
```bash
# Create on-demand backup
gcloud sql backups create \
    --instance=my-postgres \
    --description="Manual backup before migration"

# List backups
gcloud sql backups list --instance=my-postgres

# Describe backup
gcloud sql backups describe BACKUP_ID \
    --instance=my-postgres
```

### Restore from Backup
```bash
# Restore to same instance
gcloud sql backups restore BACKUP_ID \
    --backup-instance=my-postgres \
    --backup-id=BACKUP_ID

# Clone to new instance
gcloud sql instances clone my-postgres my-postgres-clone \
    --point-in-time='2025-01-01T12:00:00.000Z'
```

### Export/Import
```bash
# Export to Cloud Storage
gcloud sql export sql my-postgres \
    gs://my-bucket/export.sql \
    --database=mydb

# Import from Cloud Storage
gcloud sql import sql my-postgres \
    gs://my-bucket/import.sql \
    --database=mydb

# Export CSV
gcloud sql export csv my-postgres \
    gs://my-bucket/users.csv \
    --database=mydb \
    --query="SELECT * FROM users"
```

## High Availability

### Enable HA
```bash
gcloud sql instances patch my-postgres \
    --availability-type=REGIONAL
```

### Failover
```bash
# Trigger manual failover
gcloud sql instances failover my-postgres
```

## Read Replicas

### Create Read Replica
```bash
# Same region
gcloud sql instances create my-postgres-replica \
    --master-instance-name=my-postgres \
    --tier=db-n1-standard-1 \
    --region=us-central1

# Cross-region
gcloud sql instances create my-postgres-replica-asia \
    --master-instance-name=my-postgres \
    --tier=db-n1-standard-1 \
    --region=asia-southeast1

# Promote replica to standalone
gcloud sql instances promote-replica my-postgres-replica
```

## Performance Tuning

### Database Flags
```bash
# Set configuration parameters
gcloud sql instances patch my-postgres \
    --database-flags=\
max_connections=200,\
shared_buffers=256MB,\
effective_cache_size=1GB,\
work_mem=4MB,\
maintenance_work_mem=64MB

# Common flags:
# max_connections: Maximum concurrent connections
# shared_buffers: RAM for caching (25% of RAM)
# effective_cache_size: OS cache estimate
# work_mem: Per-operation memory
# maintenance_work_mem: VACUUM, CREATE INDEX memory
# random_page_cost: SSD=1.1, HDD=4
# log_statement: Log queries (all, ddl, mod, none)
```

### Query Insights
```bash
# Enable Query Insights
gcloud sql instances patch my-postgres \
    --insights-config-query-insights-enabled \
    --insights-config-query-string-length=4500
```

### Monitoring Queries
```sql
-- Active connections
SELECT * FROM pg_stat_activity;

-- Long running queries
SELECT pid, now() - pg_stat_activity.query_start AS duration, query
FROM pg_stat_activity
WHERE state != 'idle'
ORDER BY duration DESC;

-- Kill long running query
SELECT pg_terminate_backend(pid);

-- Table sizes
SELECT schemaname, tablename,
       pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Index usage
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan;

-- Cache hit ratio
SELECT sum(heap_blks_read) as heap_read,
       sum(heap_blks_hit) as heap_hit,
       sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
FROM pg_statio_user_tables;
```

## Security

### SSL/TLS Configuration
```bash
# Require SSL
gcloud sql instances patch my-postgres \
    --require-ssl

# Download server certificate
gcloud sql ssl-certs create client-cert client-key \
    --instance=my-postgres

# Connect with SSL
psql "host=INSTANCE_IP dbname=mydb user=postgres sslmode=verify-ca sslcert=client-cert.pem sslkey=client-key.pem sslrootcert=server-ca.pem"
```

### IAM Database Authentication
```bash
# Enable IAM authentication
gcloud sql instances patch my-postgres \
    --database-flags=cloudsql.iam_authentication=on

# Create IAM user
gcloud sql users create user@project.iam \
    --instance=my-postgres \
    --type=CLOUD_IAM_USER

# Grant IAM role
gcloud projects add-iam-policy-binding PROJECT_ID \
    --member=user:user@example.com \
    --role=roles/cloudsql.client
```

### Authorized Networks
```bash
# Add authorized network
gcloud sql instances patch my-postgres \
    --authorized-networks=203.0.113.0/24

# Remove public IP
gcloud sql instances patch my-postgres \
    --no-assign-ip
```

## Maintenance and Updates

### Maintenance Window
```bash
# Set maintenance window
gcloud sql instances patch my-postgres \
    --maintenance-window-day=SUN \
    --maintenance-window-hour=3

# Deny maintenance period
gcloud sql instances patch my-postgres \
    --deny-maintenance-period-start-date=2025-12-15 \
    --deny-maintenance-period-end-date=2026-01-05
```

### Version Upgrades
```bash
# Upgrade PostgreSQL version
gcloud sql instances patch my-postgres \
    --database-version=POSTGRES_15
```

## Monitoring

### Cloud Monitoring Metrics
- CPU utilization
- Memory utilization
- Disk read/write operations
- Network bytes sent/received
- Database connections
- Replication lag
- Transaction count

### Alerts
```bash
# Example: Create CPU alert
gcloud alpha monitoring policies create \
    --notification-channels=CHANNEL_ID \
    --display-name="High CPU Alert" \
    --condition-display-name="CPU > 80%" \
    --condition-threshold-value=0.8 \
    --condition-threshold-duration=300s
```

### Logs
```bash
# View logs
gcloud logging read "resource.type=cloudsql_database" \
    --limit 50 \
    --format json

# PostgreSQL error logs
gcloud logging read "resource.type=cloudsql_database AND severity=ERROR"
```

## Cost Optimization

### Instance Sizing
```bash
# Downsize instance
gcloud sql instances patch my-postgres \
    --tier=db-f1-micro

# Common tiers:
# db-f1-micro: 0.6GB RAM (shared CPU)
# db-g1-small: 1.7GB RAM (shared CPU)
# db-n1-standard-1: 3.75GB RAM (1 vCPU)
# db-n1-standard-2: 7.5GB RAM (2 vCPU)
# db-n1-highmem-2: 13GB RAM (2 vCPU)
```

### Storage Management
```bash
# Increase storage
gcloud sql instances patch my-postgres \
    --storage-size=20GB

# Enable storage auto-resize
gcloud sql instances patch my-postgres \
    --storage-auto-increase \
    --storage-auto-increase-limit=100
```

## Common Interview Questions

**Q: What is Cloud SQL?**
- Fully managed relational database service
- Supports PostgreSQL, MySQL, SQL Server
- Handles backups, replication, patches
- High availability and scalability

**Q: How to achieve high availability?**
- Regional configuration (multi-zone)
- Automatic failover to standby
- Synchronous replication
- 99.95% uptime SLA

**Q: Explain read replicas**
- Asynchronous replication from primary
- Offload read workloads
- Can be cross-region
- Can be promoted to standalone instance

**Q: How to secure Cloud SQL?**
- Private IP (VPC)
- SSL/TLS encryption
- IAM authentication
- Authorized networks
- Encryption at rest (default)

**Q: Backup and recovery options?**
- Automated daily backups
- On-demand backups
- Point-in-time recovery
- Export to Cloud Storage
- 7-365 days retention

**Q: How to connect from GKE?**
- Cloud SQL Proxy sidecar
- Private IP (if in same VPC)
- Workload Identity for authentication
- Connection pooling recommended

**Q: Difference between public and private IP?**
- Public: Internet-accessible (with SSL)
- Private: VPC-only access
- Private is more secure
- Private requires VPC peering

**Q: How to monitor performance?**
- Cloud Monitoring metrics
- Query Insights
- pg_stat_* views
- Slow query logs
- Connection pooling stats

**Q: What is Cloud SQL Proxy?**
- Secure tunnel to Cloud SQL
- Handles authentication
- Encrypts traffic
- No need to whitelist IPs

**Q: How to handle migrations?**
- Database Migration Service (DMS)
- Export/import via Cloud Storage
- pg_dump/pg_restore
- Logical replication
- Minimize downtime with read replicas

**Q: Cost considerations?**
- Instance tier (CPU/RAM)
- Storage (SSD vs HDD)
- Backups (included)
- Network egress
- HA adds cost
- Read replicas add cost

**Q: Scaling strategies?**
- Vertical: Increase instance tier
- Horizontal: Add read replicas
- Connection pooling (PgBouncer)
- Partitioning large tables
- Caching layer (Redis/Memcached)
